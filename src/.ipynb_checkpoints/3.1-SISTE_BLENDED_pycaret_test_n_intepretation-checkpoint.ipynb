{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "%matplotlib inline\n",
    "from pycaret.classification import *\n",
    "from pycaret.utils import check_metric\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOBAL PATH VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook folder\n",
    "NB_DIR = %pwd\n",
    "NB_DIR = Path(NB_DIR)\n",
    "\n",
    "# Root MCI foler\n",
    "ROOT_DIR = NB_DIR.parent\n",
    "\n",
    "# Main data folder (with downloaded csv files)\n",
    "MAIN_DATA_DIR = ROOT_DIR/'data'\n",
    "DATA_DIR_FS = ROOT_DIR / 'data_FS'\n",
    "\n",
    "# Current data dir with sMCI_cAD.csv & bl.csv files\n",
    "CURRENT_DATA_DIR = ROOT_DIR/'results'\n",
    "\n",
    "# Results folder\n",
    "RESULTS_DIR = ROOT_DIR/'results' #misclassified patient table\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMOPRTS TO CREATE CONFUSION MATRIX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages needed\n",
    "import mci_rf_bl as mrfbl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting to displaying all columns in pandas df\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generated in RandomForest-notebook\n",
    "dataset = pd.read_csv(RESULTS_DIR /'2.0-SISTE_random_forest_train_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.loc[dataset.Usage_=='train']\n",
    "data_unseen = dataset.loc[dataset.Usage_=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unseen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data for Modeling: ' + str(data.shape))\n",
    "print('Unseen Data For Predictions: ' + str(data_unseen.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PyCaret model created in `3.0-pycaret_pipeline`-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denne modellen er den vi ikke har pipelinen til...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = load_model(RESULTS_DIR /'best_blended_accuracy_top5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using this model to predict on unseen test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ensemble = predict_model(ensemble_model, data=data_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ensemble.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_metric(preds_ensemble['Subgroup_num_'], preds_ensemble['Label'], metric = 'Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Finn tp, fp, fn, tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unseen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ensemble.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding column `Ens_pred` for further comparison with predictions from Random Forest model (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ensemble['Ens_pred'] = \"\" \n",
    "\n",
    "for i in preds_ensemble.index:\n",
    "    \n",
    "    if preds_ensemble.loc[i,'Ens_pred'] == \"\":\n",
    "\n",
    "        if preds_ensemble.loc[i, 'Subgroup_'] == 'sMCI' and preds_ensemble.loc[i, 'Label'] == 0:\n",
    "            preds_ensemble.loc[i, 'Ens_pred'] = 'TN_'\n",
    "\n",
    "        elif preds_ensemble.loc[i, 'Subgroup_'] == 'sMCI' and preds_ensemble.loc[i, 'Label'] == 1:\n",
    "            preds_ensemble.loc[i, 'Ens_pred'] ='FP_' \n",
    "\n",
    "        elif preds_ensemble.loc[i, 'Subgroup_'] == 'cAD' and preds_ensemble.loc[i, 'Label'] == 0:\n",
    "            preds_ensemble.loc[i, 'Ens_pred'] = 'FN_' \n",
    "\n",
    "        elif preds_ensemble.loc[i, 'Subgroup_'] == 'cAD' and preds_ensemble.loc[i, 'Label'] == 1:\n",
    "            preds_ensemble.loc[i, 'Ens_pred'] ='TP_' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual inspection for minimum one of each prediction (aka TN_, FP_ FN_ & TP_)\n",
    "# If 'Subgroup_' == 'sMCI' and 'Label' == 1, then 'Ens_pred' should be 'FP_'\n",
    "preds_ensemble.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive and negative predictions\n",
    "p = preds_ensemble.loc[preds_ensemble.Label == 1]\n",
    "n = preds_ensemble.loc[preds_ensemble.Label == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = len(preds_ensemble.loc[preds_ensemble['Ens_pred'] == 'TN_'])\n",
    "fp = len(preds_ensemble.loc[preds_ensemble['Ens_pred'] == 'FP_'])\n",
    "fn = len(preds_ensemble.loc[preds_ensemble['Ens_pred'] == 'FN_'])\n",
    "tp = len(preds_ensemble.loc[preds_ensemble['Ens_pred'] == 'TP_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misclassification FP + FN / TP + TN + FP + FN\n",
    "misclass_perc = (fp + fn) / (tp + tn + fp + fn)\n",
    "print(f\"Percentage of test set misclassified: {misclass_perc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy (TP + TN) / (TP + TN + FP + FN)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(f\"Accuracyen for classification on test set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision TP / TP + FP\n",
    "precision = tp / (tp + fp)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity aka Recall (true positives / all actual positives) = TP / TP + FN\n",
    "recall = tp / (tp + fn)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity TP / (TP + FN)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specificity (true negatives / all actual negatives) = TN / TN + FP\n",
    "specificity = tn / (tn + fp)\n",
    "print(specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating confusion matrix for Ensemble's prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepareing 'y_true' from ensemble's prediction --> ground truth\n",
    "y_test = preds_ensemble.Subgroup_num_\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing y_test_pred from ensemble --> 'Label' column\n",
    "y_test_pred = preds_ensemble.Label\n",
    "y_test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting confusion matrix for test set \n",
    "conf_matrix_test_ens  = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "conf_matrix_test_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of test confusion matix \n",
    "# conf_mat_mean = conf_matrix_test_ens.mean(axis=0)\n",
    "# percantage values of confusion matix according to validatin set lenght\n",
    "\n",
    "conf_matrix_test_prc = conf_matrix_test_ens / y_test.shape[0] * 100\n",
    "\n",
    "conf_mat_ens = mrfbl.plot_confusion_matrix_TEST_IR(conf_matrix_test_ens, conf_matrix_test_prc,\n",
    "                                    file_name_number=\"K50\", title=\"Ensemble model\",\n",
    "                                    file_name_prefix=\"3.1-ensemble-conf-matrix\",\n",
    "                                    save=True, results_dir=RESULTS_DIR/'figs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data with predictions from Random Forest for further comparison. \n",
    "### File: `results/RandomForest-CV50-predictions.csv`\n",
    "Loading data frame containing information about the Random Forest model's classification on test set. \n",
    "\n",
    "Prediction (i.e TN, FP, FN or TP) is specified by `CM_pred_`-column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = pd.read_csv(RESULTS_DIR / '2.0-SISTE_random_forest-TEST-predictions.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to features needed for further analysis, all meta data is in 'Preds_ensemble'-file\n",
    "rf_pred = rf_pred[[\"RID\", \"CM_pred_\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(rf_pred, preds_ensemble, left_index=True, right_index=True)\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv( RESULTS_DIR / '3.1-SISTE_BLENDED_random_forest_n_ensemble_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking confusion matrix from Random Forest prediction\n",
    "TN_teller = 0\n",
    "FP_teller = 0\n",
    "FN_teller = 0\n",
    "TP_teller = 0\n",
    "\n",
    "for i in final_df.index:\n",
    "    if final_df.loc[i,'CM_pred_'] == 'TN':\n",
    "        TN_teller += 1\n",
    "    elif final_df.loc[i,'CM_pred_'] == 'FP':\n",
    "        FP_teller += 1\n",
    "    elif final_df.loc[i,'CM_pred_'] == 'FN':\n",
    "        FN_teller += 1\n",
    "    elif final_df.loc[i,'CM_pred_'] == 'TP':\n",
    "        TP_teller += 1\n",
    "        \n",
    "print(f\"Classification labels from Random Forest prediction on test set:\") \n",
    "print(f\"True Negatives: {TN_teller}\")\n",
    "print(f\"False Positives: {FP_teller}\")\n",
    "print(f\"False Negatives: {FN_teller}\")\n",
    "print(f\"True Positives: {TP_teller}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking confusion matrix from ensemble prediction\n",
    "tn_teller = 0\n",
    "fp_teller = 0\n",
    "fn_teller = 0\n",
    "tp_teller = 0\n",
    "\n",
    "for i in final_df.index:\n",
    "    if final_df.loc[i,'Ens_pred'] == 'TN_':\n",
    "        tn_teller += 1\n",
    "    elif final_df.loc[i,'Ens_pred'] == 'FP_':\n",
    "        fp_teller += 1\n",
    "    elif final_df.loc[i,'Ens_pred'] == 'FN_':\n",
    "        fn_teller += 1\n",
    "    elif final_df.loc[i,'Ens_pred'] == 'TP_':\n",
    "        tp_teller += 1\n",
    "        \n",
    "print(f\"Classification labels from ensemble model on test set:\") \n",
    "print(f\"True Negatives: {tn_teller}\")\n",
    "print(f\"False Positives: {fp_teller}\")\n",
    "print(f\"False Negatives: {fn_teller}\")\n",
    "print(f\"True Positives: {tp_teller}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating the two models' overlap in misclassfications of **sMCI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_overlap = 0 \n",
    "for i in final_df.index:\n",
    "    if final_df.loc[i,'Ens_pred'] == 'FP_' and final_df.loc[i,'CM_pred_'] == 'FP':\n",
    "        fp_overlap += 1\n",
    "        \n",
    "print(\"*\"*90)\n",
    "print(f\"Random Forest: misclassified {FP_teller} sMCI subjects as cAD.\")\n",
    "print(f\"Ensemblet: misclassified {fp_teller} sMCI subjects as converters.\")\n",
    "print()\n",
    "print(f\"Of these misclassifications the models overlapped {fp_overlap} deltagere.\")\n",
    "print(\"*\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying indexes for FP subjects where the models did not overlap  (sMCI --> FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misclassified as sMCI by the Ensemble and correctly classified by the Random Forest\n",
    "indeksList_fp_ensemble = []\n",
    "\n",
    "for i in final_df.index:\n",
    "    if final_df.loc[i,'Ens_pred'] == 'FP_' and final_df.loc[i,'CM_pred_'] == 'TN':\n",
    "        indeksList_fp_ensemble.append(i)\n",
    "        \n",
    "print(\"*\"*100)      \n",
    "print(f\"Subjects with the {len(indeksList_fp_ensemble)} following indices {indeksList_fp_ensemble} were correctly\")\n",
    "print(\"classified by the RF and missclassified by the ensemble\")\n",
    "print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misclassified as sMCI by the Random Forest and correctly by the Ensemble\n",
    "indeksList_fp_rf = []\n",
    "for i in final_df.index:\n",
    "    if final_df.loc[i,'Ens_pred'] == 'TN_' and final_df.loc[i,'CM_pred_'] == 'FP':\n",
    "        indeksList_fp_rf.append(i)\n",
    "        \n",
    "print(\"*\"*100)\n",
    "print(f\"Subjects with the {len(indeksList_fp_rf)} following indices {indeksList_fp_rf} were correctly\")\n",
    "print(\"classified by the ensemble and miscassified by the Random Forest\")\n",
    "print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating the two models' overlap i misclassfications of **cAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_overlap = 0 \n",
    "\n",
    "for i in final_df.index:\n",
    "    if final_df.loc[i,'Ens_pred'] == 'FN_' and final_df.loc[i,'CM_pred_'] == 'FN':\n",
    "        fn_overlap += 1\n",
    "        \n",
    "print(\"*\"*90)\n",
    "print(f\"Random Forest: misclassified {FN_teller} cAD subjects as stabile.\")\n",
    "print(f\"Ensemblet: misclassified {fn_teller} cAD as stabile.\")\n",
    "print()\n",
    "print(f\"Of these misclassifications, the models overlapped on {fn_overlap} subjects.\")\n",
    "print(\"*\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying indexes for FP subjects where the models did not overlap (cAD --> FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjects misclassified as FN by ensemble, and correctly classified as TP by the random forest:\n",
    "indeksList_fn_ensemble = []\n",
    "for i in final_df.index:\n",
    "    if final_df.loc[i,'Ens_pred'] == 'FN_' and final_df.loc[i,'CM_pred_'] == 'TP':\n",
    "        indeksList_fn_ensemble.append(i)\n",
    "        \n",
    "print(\"*\"*100)\n",
    "\n",
    "print(f\"Subjects with the {len(indeksList_fn_ensemble)} following indecies {indeksList_fn_ensemble} were correctly\")\n",
    "print(\"classified by RF and misclassified by the ensemblet\")\n",
    "print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjects misclassified as FN by the random forest, and correctly classified as TP by the ensemble:\n",
    "indeksList_fn_rf = []\n",
    "for i in final_df.index:\n",
    "    if final_df.loc[i,'Ens_pred'] == 'TP_' and final_df.loc[i,'CM_pred_'] == 'FN':\n",
    "        indeksList_fn_rf.append(i)\n",
    "        \n",
    "print(\"*\"*100)\n",
    "print(f\"Subjects with the {len(indeksList_fn_rf)} following {indeksList_fn_rf} were ble correctly\")\n",
    "print(\"classified av ensemblet, men feilaktig klassifisert av RF\")\n",
    "print(\"*\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mciCopy",
   "language": "python",
   "name": "mcicopy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
